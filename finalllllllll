'''
*****************************************************************************************
*
*        		===============================================
*           		GeoGuide(GG) Theme (eYRC 2023-24)
*        		===============================================
*
*  This script is to implement Task 1A of GeoGuide(GG) Theme (eYRC 2023-24).
*  
*  This software is made available on an "AS IS WHERE IS BASIS".
*  Licensee/end user indemnifies and will keep e-Yantra indemnified from
*  any and all claim(s) that emanate from the use of the Software or 
*  breach of the terms of this agreement.
*
*****************************************************************************************
'''

# Team ID:			[ Team-ID ]
# Author List:		[ Names of team members worked on this file separated by Comma: Name1, Name2, ... ]
# Filename:			task_1a.py
# Functions:	    [`load_dataset`, `data_preprocessing`, `identify_features_and_targets`,
# 					 `data_scaling`, `load_as_tensors`, `Salary_Predictor`, `train_model`,
# 					 `evaluate_model` ]

####################### IMPORT MODULES #######################
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from torch.utils.data import DataLoader, TensorDataset
###################### Additional Imports ####################
'''
You can import any additional modules that you require from 
torch, matplotlib or sklearn. 
You are NOT allowed to import any other libraries. It will 
cause errors while running the executable
'''
##############################################################

################# ADD UTILITY FUNCTIONS HERE #################

# Load your dataset (replace 'your_dataset.csv' with your actual dataset file)
def load_dataset(file_path):
    # Load dataset using pandas
    data = pd.read_csv(file_path)
    return data

##############################################################

# Data Preprocessing
def data_preprocessing(data):
    '''
    Purpose:
    ---
    This function will be used to load your csv dataset and preprocess it.
    Preprocessing involves cleaning the dataset by removing unwanted features,
    decision about what needs to be done with missing values etc. Note that 
    there are features in the csv file whose values are textual (eg: Industry, 
    Education Level etc) These features might be required for training the model
    but can not be given directly as strings for training. Hence this function 
    should return encoded dataframe in which all the textual features are 
    numerically labeled.

    Input Arguments:
    ---
    `data`: [Dataframe]
           Pandas dataframe read from the provided dataset 

    Returns:
    ---
    `data`: [ Dataframe ]
           Pandas dataframe that has all the features mapped to 
           numbers starting from zero

    Example call:
    ---
    data = data_preprocessing(data)
    '''

    #################    ADD YOUR CODE HERE    ##################
    # Encode categorical variables using OneHotEncoder
    categorical_columns = ['Education', 'City', 'Gender', 'EverBenched']
    
    onehot_encoder = OneHotEncoder(sparse=False)
    for column in categorical_columns:
        encoded_columns = onehot_encoder.fit_transform(data[column].values.reshape(-1, 1))
        encoded_df = pd.DataFrame(encoded_columns, columns=[f'{column}_{i}' for i in range(encoded_columns.shape[1])])
        data = pd.concat([data, encoded_df], axis=1)
        data.drop([column], axis=1, inplace=True)

    # Handle missing values (you can choose a suitable strategy)
    data.fillna(0, inplace=True)  # Replace missing values with 0 for demonstration

    return data

##############################################################

# Define features and target
def identify_features_and_targets(data):
    '''
    Purpose:
    ---
    The purpose of this function is to define the features and
    the required target labels. The function returns a python list
    in which the first item is the selected features and second 
    item is the target label

    Input Arguments:
    ---
    `data` : [ Dataframe ]
             Pandas dataframe that has all the features mapped to 
             numbers starting from zero

    Returns:
    ---
    `features_and_targets` : [ list ]
                            python list in which the first item is the 
                            selected features and second item is the target label

    Example call:
    ---
    features_and_targets = identify_features_and_targets(data)
    '''

    #################    ADD YOUR CODE HERE    ##################
    X = data.drop(columns=['LeaveOrNot'])
    y = data['LeaveOrNot']
    return X, y

##############################################################

# Data Scaling (Standardization)
def data_scaling(X_train, X_test):
    '''
    Purpose:
    ---
    This function scales the training and testing data using StandardScaler.

    Input Arguments:
    ---
    `X_train` : [array-like]
                Training features

    `X_test` : [array-like]
               Testing features

    Returns:
    ---
    `X_train_scaled` : [array-like]
                       Scaled training features

    `X_test_scaled` : [array-like]
                      Scaled testing features

    Example call:
    ---
    X_train_scaled, X_test_scaled = data_scaling(X_train, X_test)
    '''

    #################    ADD YOUR CODE HERE    ##################
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    return X_train_scaled, X_test_scaled

##############################################################

# Convert data to PyTorch tensors
def load_as_tensors(X_train, y_train, X_test, y_test):
    '''
    Purpose:
    ---
    This function loads your data (both training and validation)
    as PyTorch tensors. Here you will have to split the dataset for training 
    and validation, and then load them as tensors. 
    Training of the model requires iterating over the training tensors. 
    Hence the training sensors need to be converted to iterable dataset
    object.

    Input Arguments:
    ---
    `X_train` : [array-like]
                Training features

    `y_train` : [array-like]
                Training labels

    `X_test` : [array-like]
               Testing features

    `y_test` : [array-like]
               Testing labels

    Returns:
    ---
    `tensors_and_iterable_training_data` : [ list ]
                                            Items:
                                            [0]: X_train_tensor: Training features loaded into Pytorch array
                                            [1]: X_test_tensor: Feature tensors in validation data
                                            [2]: y_train_tensor: Training labels as Pytorch tensor
                                            [3]: y_test_tensor: Target labels as tensor in validation data
                                            [4]: Iterable dataset object and iterating over it in 
                                                 batches, which are then fed into the model for processing

    Example call:
    ---
    tensors_and_iterable_training_data = load_as_tensors(X_train, y_train, X_test, y_test)
    '''

    #################    ADD YOUR CODE HERE    ##################
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)
    return X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor

##############################################################

# Define the neural network model
class Salary_Predictor(nn.Module):
    def __init__(self, input_size):
        super(Salary_Predictor, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        x = self.sigmoid(x)
        return x

##############################################################

# Training function
def train_model(model, X_train_tensor, y_train_tensor, num_epochs, optimizer, loss_function):
    '''
    Purpose:
    ---
    This function trains the neural network model.

    Input Arguments:
    ---
    `model` : [nn.Module]
              Neural network model

    `X_train_tensor` : [torch.Tensor]
                       Training features loaded into PyTorch tensor

    `y_train_tensor` : [torch.Tensor]
                       Training labels as PyTorch tensor

    `num_epochs` : [int]
                   Number of training epochs

    `optimizer` : [torch.optim]
                  Optimizer for updating model parameters

    `loss_function` : [torch.nn]
                      Loss function for the model

    Returns:
    ---
    None

    Example call:
    ---
    train_model(model, X_train_tensor, y_train_tensor, num_epochs, optimizer, loss_function)
    '''

    #################    ADD YOUR CODE HERE    ##################
    model.train()
    
    for epoch in range(num_epochs):
        optimizer.zero_grad()
        outputs = model(X_train_tensor)
        loss = loss_function(outputs, y_train_tensor.view(-1, 1))
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')

##############################################################

# Evaluation function
def evaluate_model(model, X_test_tensor, y_test_tensor):
    '''
    Purpose:
    ---
    This function evaluates the trained model on the testing data.

    Input Arguments:
    ---
    `model` : [nn.Module]
              Trained neural network model

    `X_test_tensor` : [torch.Tensor]
                      Testing features loaded into PyTorch tensor

    `y_test_tensor` : [torch.Tensor]
                      Testing labels as PyTorch tensor

    Returns:
    ---
    `accuracy` : [float]
                 Accuracy on the testing data

    Example call:
    ---
    accuracy = evaluate_model(model, X_test_tensor, y_test_tensor)
    '''

    #################    ADD YOUR CODE HERE    ##################
    with torch.no_grad():
        model.eval()
        test_outputs = model(X_test_tensor)
        predicted = (test_outputs >= 0.5).float()
        accuracy = (predicted == y_test_tensor.view(-1, 1)).sum().item() / len(y_test_tensor)
        return accuracy * 100

##############################################################

if __name__ == "__main__":
    # Load and preprocess the dataset
    file_path = 'task_1a_dataset.csv'
    data = load_dataset(file_path)
    preprocessed_data = data_preprocessing(data)
    
    # Identify features and targets
    X, y = identify_features_and_targets(preprocessed_data)
    
    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Data scaling
    X_train_scaled, X_test_scaled = data_scaling(X_train, X_test)
    
    # Convert data to PyTorch tensors
    X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = load_as_tensors(X_train_scaled, y_train, X_test_scaled, y_test)
    
    # Initialize the model
    input_size = X_train_tensor.shape[1]
    model = Salary_Predictor(input_size)
    
    # Define loss function and optimizer
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # Training
    num_epochs = 100
    train_model(model, X_train_tensor, y_train_tensor, num_epochs, optimizer, criterion)
    
    # Evaluation
    accuracy = evaluate_model(model, X_test_tensor, y_test_tensor)
    print(f'Test Accuracy: {accuracy:.2f}%')
    
    # Save the trained model if needed
    torch.save(model.state_dict(), 'task_1a_trained_model.pth')
